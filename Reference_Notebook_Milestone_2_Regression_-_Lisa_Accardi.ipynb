{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKrvyhfVL3e"
   },
   "source": [
    "# **Milestone 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzh8pcvmtOS8"
   },
   "source": [
    "## **Model Building**\n",
    "\n",
    "1. What we want to predict is the \"Price\". We will use the normalized version 'price_log' for modeling.\n",
    "2. Before we proceed to the model, we'll have to encode categorical features. We will drop categorical features like Name. \n",
    "3. We'll split the data into train and test, to be able to evaluate the model that we build on the train data.\n",
    "4. Build Regression models using train data.\n",
    "5. Evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please load the data frame that was saved in Milestone 1 here before separating the data, and then proceed to the next step in Milestone 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.metrics \n",
    "import sklearn.metrics as metrics  \n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve,recall_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cars_data = pd.read_csv(\"cars_data_updated.csv\")\n",
    "cars_data.head()\n",
    "cars_data = cars_data.fillna(method='ffill')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L-oAMItxLP-"
   },
   "source": [
    "### **Split the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aat-Ne-ZVL3e"
   },
   "source": [
    "<li>Step1: Seperating the indepdent variables (X) and the dependent variable (y). \n",
    "<li>Step2: Encode the categorical variables in X using pd.dummies.\n",
    "<li>Step3: Split the data into train and test using train_test_split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cwh_IhfqVL3f"
   },
   "source": [
    "**Think about it:** Why we should drop 'Name','Price','price_log','Kilometers_Driven' from X before splitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NTly1jIxtOS8"
   },
   "outputs": [],
   "source": [
    "# Step-1\n",
    "X = cars_data.drop(['Name','Price','Price_log','Kilometers_Driven', 'kilometers_driven_log', 'New_price_log', 'Power_log', 'New_price'], axis = 1)\n",
    "\n",
    "y = cars_data[[\"Price_log\", \"Price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vzCLGMzbVL3f"
   },
   "outputs": [],
   "source": [
    "# Step-2 Use pd.get_dummies(drop_first = True)\n",
    "X = pd.get_dummies(X, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JqVHLEHVRRKK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5076, 24) (2176, 24)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Step-3 Splitting data into training and test set:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "glAH2vMFtOS8"
   },
   "outputs": [],
   "source": [
    "# # Let us write a function for calculating r2_score and RMSE on train and test data\n",
    "# This function takes model as an input on which we have trained particular algorithm\n",
    "# The categorical column as the input and returns the boxplots and histograms for the variable\n",
    "\n",
    "def get_model_score(model, flag = True):\n",
    "    '''\n",
    "    model : regressor to predict values of X\n",
    "\n",
    "    '''\n",
    "    # Defining an empty list to store train and test results\n",
    "    score_list = [] \n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    \n",
    "    pred_train_ = np.exp(pred_train)\n",
    "    \n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    pred_test_ = np.exp(pred_test)\n",
    "    \n",
    "    train_r2 = metrics.r2_score(y_train['Price'], pred_train_)\n",
    "    \n",
    "    test_r2 = metrics.r2_score(y_test['Price'], pred_test_)\n",
    "    \n",
    "    train_rmse = metrics.mean_squared_error(y_train['Price'], pred_train_, squared = False)\n",
    "    \n",
    "    test_rmse = metrics.mean_squared_error(y_test['Price'], pred_test_, squared = False)\n",
    "    \n",
    "    # Adding all scores in the list\n",
    "    score_list.extend((train_r2, test_r2, train_rmse, test_rmse))\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed, the default value is True\n",
    "    if flag == True: \n",
    "        \n",
    "        print(\"R-sqaure on training set : \", metrics.r2_score(y_train['Price'], pred_train_))\n",
    "        \n",
    "        print(\"R-square on test set : \", metrics.r2_score(y_test['Price'], pred_test_))\n",
    "        \n",
    "        print(\"RMSE on training set : \", np.sqrt(metrics.mean_squared_error(y_train['Price'], pred_train_)))\n",
    "        \n",
    "        print(\"RMSE on test set : \", np.sqrt(metrics.mean_squared_error(y_test['Price'], pred_test_)))\n",
    "    \n",
    "    # Returning the list with train and test scores\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8qcI692VL3g"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaj2riZFVL3g"
   },
   "source": [
    "For Regression Problems, some of the algorithms used are :<br>\n",
    "\n",
    "**1) Linear Regression** <br>\n",
    "**2) Ridge / Lasso Regression** <br>\n",
    "**3) Decision Trees** <br>\n",
    "**4) Random Forest** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwL33RaztOS9"
   },
   "source": [
    "### **Fitting a linear model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXj-84YCVL3h"
   },
   "source": [
    "Linear Regression can be implemented using: <br>\n",
    "\n",
    "**1) Sklearn:** https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html <br>\n",
    "**2) Statsmodels:** https://www.statsmodels.org/stable/regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tABeKbbNVL3h"
   },
   "outputs": [],
   "source": [
    "# Import Linear Regression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FT3gcKDetOS9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7252 entries, 0 to 7251\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   S.No.                  7252 non-null   int64  \n",
      " 1   Name                   7252 non-null   object \n",
      " 2   Location               7252 non-null   object \n",
      " 3   Year                   7252 non-null   int64  \n",
      " 4   Kilometers_Driven      7252 non-null   int64  \n",
      " 5   Fuel_Type              7252 non-null   object \n",
      " 6   Transmission           7252 non-null   object \n",
      " 7   Owner_Type             7252 non-null   object \n",
      " 8   Mileage                7252 non-null   float64\n",
      " 9   Engine                 7252 non-null   float64\n",
      " 10  Power                  7252 non-null   float64\n",
      " 11  Seats                  7252 non-null   float64\n",
      " 12  New_price              7250 non-null   float64\n",
      " 13  Price                  7252 non-null   float64\n",
      " 14  kilometers_driven_log  7252 non-null   float64\n",
      " 15  Price_log              7252 non-null   float64\n",
      " 16  Power_log              7252 non-null   float64\n",
      " 17  New_price_log          7252 non-null   float64\n",
      "dtypes: float64(10), int64(3), object(5)\n",
      "memory usage: 1019.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create a linear regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "cars_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rMmX-FJatOS9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear regression model\n",
    "lr.fit(X_train, y_train['Price_log']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ABshmMPAtOS9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.6026935985967086\n",
      "R-square on test set :  0.7010999697209037\n",
      "RMSE on training set :  6.608285256712908\n",
      "RMSE on test set :  5.765673746214778\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model\n",
    "LR_score = get_model_score(lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgNkZ0HctOS9"
   },
   "source": [
    "**Observations from results: \n",
    "* The R-square value on the training set is lower than on the actual test set, but still not a great fit.\n",
    "* The RMSE on both the training set and the test set are not similar at all to the R-square value. \n",
    "* We do not have a well fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNN1nlPqIrdC"
   },
   "source": [
    "**Important variables of Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyKxdVyhVL3i"
   },
   "source": [
    "Building a model using statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vHyqhuVMIrdC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              Price_log   R-squared:                       0.683\n",
      "Model:                            OLS   Adj. R-squared:                  0.682\n",
      "Method:                 Least Squares   F-statistic:                     473.3\n",
      "Date:                Thu, 28 Jul 2022   Prob (F-statistic):               0.00\n",
      "Time:                        11:29:45   Log-Likelihood:                -3547.1\n",
      "No. Observations:                5076   AIC:                             7142.\n",
      "Df Residuals:                    5052   BIC:                             7299.\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                      -214.8127      5.396    -39.811      0.000    -225.391    -204.234\n",
      "S.No.                        -0.0001   3.25e-06    -32.606      0.000      -0.000   -9.97e-05\n",
      "Year                          0.1074      0.003     39.905      0.000       0.102       0.113\n",
      "Mileage                      -0.0092      0.002     -3.822      0.000      -0.014      -0.004\n",
      "Engine                        0.0003   3.11e-05      8.402      0.000       0.000       0.000\n",
      "Power                         0.0050      0.000     16.157      0.000       0.004       0.006\n",
      "Seats                        -0.0093      0.011     -0.821      0.412      -0.031       0.013\n",
      "Location_Bangalore            0.0729      0.045      1.611      0.107      -0.016       0.162\n",
      "Location_Chennai             -0.0076      0.043     -0.177      0.859      -0.092       0.076\n",
      "Location_Coimbatore           0.0233      0.041      0.563      0.574      -0.058       0.105\n",
      "Location_Delhi               -0.0337      0.042     -0.806      0.420      -0.116       0.048\n",
      "Location_Hyderabad            0.0718      0.040      1.786      0.074      -0.007       0.151\n",
      "Location_Jaipur              -0.0760      0.044     -1.726      0.084      -0.162       0.010\n",
      "Location_Kochi               -0.0588      0.041     -1.420      0.156      -0.140       0.022\n",
      "Location_Kolkata             -0.2009      0.042     -4.775      0.000      -0.283      -0.118\n",
      "Location_Mumbai              -0.0287      0.040     -0.716      0.474      -0.107       0.050\n",
      "Location_Pune                -0.0713      0.041     -1.727      0.084      -0.152       0.010\n",
      "Fuel_Type_Diesel              0.1723      0.072      2.378      0.017       0.030       0.314\n",
      "Fuel_Type_Electric        -2.831e-16   5.36e-16     -0.529      0.597   -1.33e-15    7.67e-16\n",
      "Fuel_Type_LPG                 0.0930      0.198      0.469      0.639      -0.296       0.482\n",
      "Fuel_Type_Petrol             -0.0348      0.073     -0.476      0.634      -0.178       0.109\n",
      "Transmission_Manual          -0.2774      0.021    -13.465      0.000      -0.318      -0.237\n",
      "Owner_Type_Fourth & Above     0.1726      0.156      1.109      0.268      -0.133       0.478\n",
      "Owner_Type_Second            -0.0245      0.020     -1.197      0.231      -0.065       0.016\n",
      "Owner_Type_Third             -0.1814      0.052     -3.495      0.000      -0.283      -0.080\n",
      "==============================================================================\n",
      "Omnibus:                      925.677   Durbin-Watson:                   2.034\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2956.199\n",
      "Skew:                          -0.925   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.249   Cond. No.                     1.05e+21\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.06e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Import Statsmodels \n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Statsmodel api does not add a constant by default. We need to add it explicitly\n",
    "x_train = sm.add_constant(X_train)\n",
    "\n",
    "# Add constant to test data\n",
    "x_test = sm.add_constant(X_test)\n",
    "\n",
    "def build_ols_model(train):\n",
    "    \n",
    "    # Create the model\n",
    "    olsmodel = sm.OLS(y_train[\"Price_log\"], train)\n",
    "    \n",
    "    return olsmodel.fit()\n",
    "\n",
    "\n",
    "\n",
    "# Fit linear model on new dataset\n",
    "olsmodel1 = build_ols_model(x_train)\n",
    "\n",
    "print(olsmodel1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C01l5EucVL3i"
   },
   "outputs": [],
   "source": [
    "# Retrive Coeff values, p-values and store them in the dataframe\n",
    "olsmod = pd.DataFrame(olsmodel1.params, columns = ['coef'])\n",
    "\n",
    "olsmod['pval'] = olsmodel1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FbQQZacV9WMm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fuel_Type_Diesel</th>\n",
       "      <td>0.172315</td>\n",
       "      <td>1.742291e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Owner_Type_Third</th>\n",
       "      <td>-0.181412</td>\n",
       "      <td>4.783855e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mileage</th>\n",
       "      <td>-0.009237</td>\n",
       "      <td>1.338413e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Kolkata</th>\n",
       "      <td>-0.200940</td>\n",
       "      <td>1.849726e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine</th>\n",
       "      <td>0.000261</td>\n",
       "      <td>5.625166e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transmission_Manual</th>\n",
       "      <td>-0.277407</td>\n",
       "      <td>1.245178e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.005006</td>\n",
       "      <td>2.719168e-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No.</th>\n",
       "      <td>-0.000106</td>\n",
       "      <td>8.253516e-212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>-214.812696</td>\n",
       "      <td>1.063912e-301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>0.107408</td>\n",
       "      <td>6.141052e-303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           coef           pval\n",
       "Fuel_Type_Diesel       0.172315   1.742291e-02\n",
       "Owner_Type_Third      -0.181412   4.783855e-04\n",
       "Mileage               -0.009237   1.338413e-04\n",
       "Location_Kolkata      -0.200940   1.849726e-06\n",
       "Engine                 0.000261   5.625166e-17\n",
       "Transmission_Manual   -0.277407   1.245178e-40\n",
       "Power                  0.005006   2.719168e-57\n",
       "S.No.                 -0.000106  8.253516e-212\n",
       "const               -214.812696  1.063912e-301\n",
       "Year                   0.107408  6.141052e-303"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter by significant p-value (pval <= 0.05) and sort descending by Odds ratio\n",
    "\n",
    "olsmod = olsmod.sort_values(by = \"pval\", ascending = False)\n",
    "\n",
    "pval_filter = olsmod['pval']<= 0.05\n",
    "\n",
    "olsmod[pval_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yjplRhssIrdC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMost overall significant categorical varaibles of LINEAR REGRESSION  are \u001b[95m :\n",
      " ['Fuel_Type', 'Owner_Type', 'Mileage', 'Location', 'Engine', 'Transmission', 'Power', 'Power_log', 'S.No.', 'Year']\n"
     ]
    }
   ],
   "source": [
    "# We are looking are overall significant varaible\n",
    "\n",
    "pval_filter = olsmod['pval']<= 0.05\n",
    "mp_vars = olsmod[pval_filter].index.tolist()\n",
    "\n",
    "# We are going to get overall varaibles (un-one-hot encoded varables) from categorical varaibles\n",
    "sig_var = []\n",
    "for col in mp_vars:\n",
    "    if '' in col:\n",
    "        first_part = col.split('_')[0]\n",
    "        for c in cars_data.columns:\n",
    "            if first_part in c and c not in sig_var :\n",
    "                sig_var.append(c)\n",
    "\n",
    "                \n",
    "start = '\\033[1m'\n",
    "end = '\\033[95m'\n",
    "print(start+ 'Most overall significant categorical varaibles of LINEAR REGRESSION  are ' +end,':\\n', sig_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uubmKLlVL3j"
   },
   "source": [
    "**Build Ridge / Lasso Regression similar to Linear Regression:**<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "9--bgDNhVL3j"
   },
   "outputs": [],
   "source": [
    "# Import Ridge/ Lasso Regression from sklearn\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "TwSuk8Q9VL3j"
   },
   "outputs": [],
   "source": [
    "# Create a Ridge regression model\n",
    "\n",
    "\n",
    "rdg = Ridge(alpha = 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "5cWh1QK0VL3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Ridge regression model\n",
    "\n",
    "rdg.fit(X_train, y_train['Price_log'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "c2tIACYOVL3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.6026014404806532\n",
      "R-square on test set :  0.7011265075974136\n",
      "RMSE on training set :  6.609051632242788\n",
      "RMSE on test set :  5.765417787501582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get score of the model\n",
    "\n",
    "rdg = get_model_score(rdg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from results:\n",
    "* Results same as linear regression\n",
    "* The R-square value on the training set is lower than on the actual test set, but still not a great fit.\n",
    "* The RMSE on both the training set and the test set are not similar at all to the R-square value. \n",
    "* We do not have a well fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owyg5IpstOS9"
   },
   "source": [
    "### **Decision Tree** \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "GjZeRER6VL3k"
   },
   "outputs": [],
   "source": [
    "# Import Decision tree for Regression from sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "shBet9WztOS-"
   },
   "outputs": [],
   "source": [
    "# Create a decision tree regression model, use random_state = 1\n",
    "dtree= DecisionTreeRegressor(random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "eJxVYVXXtOS-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision tree regression model\n",
    "dtree.fit(X_train, y_train['Price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "vGbEjda0tOS-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  1.0\n",
      "R-square on test set :  0.7999924566793265\n",
      "RMSE on training set :  3.883216341005871e-15\n",
      "RMSE on test set :  4.716396106505038\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model\n",
    "Dtree_model = get_model_score(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrCgLVKwtOS-"
   },
   "source": [
    "**Observations from results: \n",
    "* The training set has a perfect score making me concerned of an overfit.\n",
    "* The test data looks good with a .799\n",
    "* The RSME looks better than the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z2V0IwtVL3k"
   },
   "source": [
    "Print the importance of features in the tree building. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "E8ro_i9vIrdF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Power                      0.452682\n",
      "S.No.                      0.249642\n",
      "Year                       0.200030\n",
      "Engine                     0.035708\n",
      "Mileage                    0.018074\n",
      "Transmission_Manual        0.007137\n",
      "Fuel_Type_Diesel           0.006103\n",
      "Seats                      0.005884\n",
      "Location_Kolkata           0.004798\n",
      "Location_Hyderabad         0.003448\n",
      "Owner_Type_Second          0.002710\n",
      "Fuel_Type_Petrol           0.002593\n",
      "Location_Coimbatore        0.002053\n",
      "Location_Bangalore         0.001908\n",
      "Location_Mumbai            0.001837\n",
      "Location_Delhi             0.001668\n",
      "Location_Jaipur            0.001046\n",
      "Location_Pune              0.000763\n",
      "Location_Kochi             0.000762\n",
      "Location_Chennai           0.000638\n",
      "Owner_Type_Third           0.000505\n",
      "Owner_Type_Fourth & Above  0.000008\n",
      "Fuel_Type_LPG              0.000003\n",
      "Fuel_Type_Electric         0.000000\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(dtree.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp',\n",
    "                                                                                                       ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9P0pzHHIrdG"
   },
   "source": [
    "**Observations and insights:\n",
    "* Variables of importance: 'Fuel_Type', 'Owner_Type', 'Location', 'Seats', 'Transmission', 'Engine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8eFynaNtOS-"
   },
   "source": [
    "### **Random Forest**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Jhw-0FsNVL3l"
   },
   "outputs": [],
   "source": [
    "# Import Randomforest for Regression from sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "-4S4FoDXtOS-"
   },
   "outputs": [],
   "source": [
    "# Create a Randomforest regression model \n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "gBlavhMTtOS-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Randomforest regression model\n",
    "regr.fit(X_train, y_train['Price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "VLDDeeAGtOS_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.42505246649018924\n",
      "R-square on test set :  0.4207492457674372\n",
      "RMSE on training set :  7.949505388751899\n",
      "RMSE on test set :  8.026392407400266\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model\n",
    "regr_score= get_model_score(regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNTRfaitOS_"
   },
   "source": [
    "**Observations and insights: \n",
    "* This model has performed the worst overall with a low R-square value and a high RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgwyNxUuIrdG"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "AWRS7zISIrdG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Power                      0.518226\n",
      "S.No.                      0.296466\n",
      "Year                       0.141604\n",
      "Engine                     0.043704\n",
      "Location_Mumbai            0.000000\n",
      "Owner_Type_Second          0.000000\n",
      "Owner_Type_Fourth & Above  0.000000\n",
      "Transmission_Manual        0.000000\n",
      "Fuel_Type_Petrol           0.000000\n",
      "Fuel_Type_LPG              0.000000\n",
      "Fuel_Type_Electric         0.000000\n",
      "Fuel_Type_Diesel           0.000000\n",
      "Location_Pune              0.000000\n",
      "Location_Kochi             0.000000\n",
      "Location_Kolkata           0.000000\n",
      "Location_Jaipur            0.000000\n",
      "Location_Hyderabad         0.000000\n",
      "Location_Delhi             0.000000\n",
      "Location_Coimbatore        0.000000\n",
      "Location_Chennai           0.000000\n",
      "Location_Bangalore         0.000000\n",
      "Seats                      0.000000\n",
      "Mileage                    0.000000\n",
      "Owner_Type_Third           0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print important features similar to decision trees\n",
    "print(pd.DataFrame(regr.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending= False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG9ZD9ozIrdH"
   },
   "source": [
    "**Observations and insights: \n",
    "* This is showing all variables of equally high importance except for 'Engine', 'Year', 'S.No.', and 'Power'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw0dMSgetOS_"
   },
   "source": [
    "### **Hyperparameter Tuning: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "4eF0aYHHtOS_"
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "All estimators failed to fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m gridCV \u001b[38;5;241m=\u001b[39m GridSearchCV(dtree_tuned, parameters, scoring \u001b[38;5;241m=\u001b[39m scorer, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fitting the grid search on the train data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m gridCV \u001b[38;5;241m=\u001b[39m \u001b[43mgridCV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrice_log\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Set the classifier to the best combination of parameters\u001b[39;00m\n\u001b[1;32m     20\u001b[0m dtree_tuned \u001b[38;5;241m=\u001b[39m gridCV\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n\u001b[0;32m--> 875\u001b[0m     \u001b[43m_insert_error_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m all_candidate_params\u001b[38;5;241m.\u001b[39mextend(candidate_params)\n\u001b[1;32m    878\u001b[0m all_out\u001b[38;5;241m.\u001b[39mextend(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:331\u001b[0m, in \u001b[0;36m_insert_error_scores\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    328\u001b[0m         successful_score \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m successful_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll estimators failed to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(successful_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    334\u001b[0m     formatted_error \u001b[38;5;241m=\u001b[39m {name: error_score \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m successful_score}\n",
      "\u001b[0;31mNotFittedError\u001b[0m: All estimators failed to fit"
     ]
    }
   ],
   "source": [
    "# Choose the type of regressor. \n",
    "dtree_tuned = DecisionTreeRegressor(random_state = 1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'max_depth': [None], \n",
    "              'min_samples_leaf': [1, 3, 5, 7],\n",
    "              'max_leaf_nodes' : [2, 5, 7] + [None],\n",
    "             }\n",
    "\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(dtree_tuned, parameters, scoring = scorer,cv = 5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train['Price_log'])\n",
    "\n",
    "# Set the model to the best combination of parameters\n",
    "dtree_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "dtree_tuned.fit(X_train, y_train['Price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "hctfJIAXtOS_"
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get score of the dtree_tuned\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdtree_tuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImp\u001b[39m\u001b[38;5;124m\"\u001b[39m], index \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39msort_values(by \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImp\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:629\u001b[0m, in \u001b[0;36mBaseDecisionTree.feature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the feature importances.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    The importance of a feature is computed as the (normalized) total\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m        (Gini importance).\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mcompute_feature_importances()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1218\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1219\u001b[0m     ]\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Get score of the dtree_tuned\n",
    "print(pd.DataFrame(dtree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending= False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hctfJIAXtOS_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsGmvq1StOS_"
   },
   "source": [
    "**Observations and insights: _____**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jssEF5eIrdH"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdzQWq8WtOTA"
   },
   "outputs": [],
   "source": [
    "# Print important features of tuned decision tree similar to decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r8AR_VotOTB"
   },
   "source": [
    "**Observations and insights: _____**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18uxHTy2tOTB"
   },
   "source": [
    "### **Hyperparameter Tuning: Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4P_Mj0JYtOTC"
   },
   "outputs": [],
   "source": [
    "# Choose the type of Regressor\n",
    "\n",
    "# Define the parameters for Grid to choose from \n",
    "\n",
    "# Check documentation for all the parametrs that the model takes and play with those\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "\n",
    "# Run the grid search\n",
    "\n",
    "# Set the model to the best combination of parameters\n",
    "\n",
    "# Fit the best algorithm to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of regressor\n",
    "\n",
    "rf_tuned = RandomForestRegressor(random_state = 1,oob_score = True)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {  \n",
    "                'max_depth':[5,7,None],\n",
    "                'max_features': ['sqrt','log2'],\n",
    "                'n_estimators': [250,500,800]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train['Price_log'])\n",
    "\n",
    "# Set the model to the best combination of parameters\n",
    "rf_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "rf_tuned.fit(X_train, y_train['Price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSBtYgpctOTC"
   },
   "outputs": [],
   "source": [
    "# Get score of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1WHqIX9tOTC"
   },
   "source": [
    "**Observations and insights: _____**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItsgSUyiIrdI"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9khvM2ZhtOTC"
   },
   "outputs": [],
   "source": [
    "# Print important features of tuned decision tree similar to decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBoHEXnjtOTC"
   },
   "source": [
    "**Observations and insights: ______**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCInk4Y8tOTC"
   },
   "outputs": [],
   "source": [
    "# Defining list of models you have trained\n",
    "models = [lr, dtree, __________________]\n",
    "\n",
    "# Defining empty lists to add train and test results\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "\n",
    "# Looping through all the models to get the rmse and r2 scores\n",
    "for model in models:\n",
    "    \n",
    "    # Accuracy score\n",
    "    j = get_model_score(model, False)\n",
    "    \n",
    "    r2_train.append(j[0])\n",
    "    \n",
    "    r2_test.append(j[1])\n",
    "    \n",
    "    rmse_train.append(j[2])\n",
    "    \n",
    "    rmse_test.append(j[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuLokC7xtOTD"
   },
   "outputs": [],
   "source": [
    "comparison_frame = pd.DataFrame({'Model':['Linear Regression','Decision Tree', ___________, ___________], \n",
    "                                          'Train_r2': r2_train,'Test_r2': r2_test,\n",
    "                                          'Train_RMSE': rmse_train,'Test_RMSE': rmse_test}) \n",
    "comparison_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZrq2E9VtOTD"
   },
   "source": [
    "**Observations: _____**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acSjU_ZFvyVt"
   },
   "source": [
    "**Note:** You can also try some other algorithms such as KNN and compare the model performance with the existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58KMVhO_tOTD"
   },
   "source": [
    "### **Insights**\n",
    "\n",
    "**Refined insights**:\n",
    "- What are the most meaningful insights from the data relevant to the problem?\n",
    "\n",
    "**Comparison of various techniques and their relative performance**:\n",
    "- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?\n",
    "\n",
    "**Proposal for the final solution design**:\n",
    "- What model do you propose to be adopted? Why is this the best solution to adopt?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a1WHqIX9tOTC",
    "PBoHEXnjtOTC",
    "TZrq2E9VtOTD"
   ],
   "name": "Reference_Notebook_Milestone_2_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
